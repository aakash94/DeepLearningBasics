{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "#plt.ion()\n",
    "\n",
    "tag_reward = \"reward\"\n",
    "tag_loss = \"loss\"\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## writer = SummaryWriter(comment='__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "r_buff_header = ['state', 'action', 'next_state', 'reward', 'done']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.header = r_buff_header\n",
    "        self.buffer = pd.DataFrame(columns=self.header)        \n",
    "        self.count_i = 0\n",
    "        self.count_p = 0\n",
    "\n",
    "    def push(self, df_row):\n",
    "        self.count_p += 1\n",
    "        if self.__len__() == self.capacity:\n",
    "            # Probably exceeded capacity\n",
    "            #remove a row (probably 1st one) here \n",
    "            self.buffer = self.buffer.iloc[1:]\n",
    "        #add to dataframe here\n",
    "        self.buffer = pd.concat([self.buffer, df_row])\n",
    "        \n",
    "        \n",
    "    def insert(self, stateV, actonV, next_stateV, rewardV, doneV):\n",
    "        # Initialise data to lists. \n",
    "        self.count_i += 1\n",
    "        data = [{self.header[0]: stateV, \n",
    "                 self.header[1]: actonV, \n",
    "                 self.header[2]: next_stateV, \n",
    "                 self.header[3]: rewardV, \n",
    "                 self.header[4]: doneV}] \n",
    "  \n",
    "        # Creates DataFrame. \n",
    "        df = pd.DataFrame(data)\n",
    "        self.push(df)\n",
    "        \n",
    "        #print(\"len = \", self.__len__(),\"\\t insert = \", self.count_i, \"\\t push = \", self.count_p)\n",
    "        \n",
    "            \n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        return self.buffer.sample(batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.buffer.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DqnAgent(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_ip, n_op):\n",
    "        super(DqnAgent, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_ip, n_ip*2)\n",
    "        self.fc2 = nn.Linear(n_ip*2, n_ip)\n",
    "        self.fc3 = nn.Linear(n_ip, n_op)       \n",
    "        \n",
    "    def forward(self,x):        \n",
    "        x=self.fc1(x)\n",
    "        x=self.fc2(x)\n",
    "        x=self.fc3(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "GAMMA      = 0.9 # discount factor\n",
    "EPSILON    = 0.9\n",
    "\n",
    "CHECK_EVERY = 100\n",
    "OPTIMIZE_EVERY = 50\n",
    "\n",
    "\n",
    "STATE_N  = 4\n",
    "ACTION_N = env.action_space.n\n",
    "\n",
    "LOG_FOLDER = \"log_res/\"\n",
    "\n",
    "# EPS_START  = 0.9\n",
    "# EPS_END    = 0.05\n",
    "# EPS_DECAY  = 200\n",
    "# TARGET_UPDATE = 10\n",
    "\n",
    "# Since target policy is absolutely greedy version of Explore policy.\n",
    "# No need of using 2 vfa nets. use 1 network.\n",
    "\n",
    "## NUM_EPISODES = 3000\n",
    "NUM_EPISODES = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "## plt.figure()\n",
    "\n",
    "eval_count = 0\n",
    "train_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "qvfa = DqnAgent(STATE_N, ACTION_N).double().to(device)\n",
    "optimizer = optim.RMSprop(qvfa.parameters())\n",
    "## memory = ReplayMemory(10000)\n",
    "buffer = ReplayBuffer(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, ep = 0):    \n",
    "    sample = random.random()\n",
    "    state = torch.from_numpy(state).to(device)\n",
    "    if sample > ep:\n",
    "        with torch.no_grad():            \n",
    "            op = qvfa(state)\n",
    "            #print(\"op \\t\" , op)\n",
    "            values, indices = op.max(0)\n",
    "            #print(\"indices \\t\",indices.item() )\n",
    "            return indices.item()\n",
    "    else:\n",
    "        return env.action_space.sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    ## clear_output(wait=True)    \n",
    "    if buffer.__len__() < BATCH_SIZE:\n",
    "        print(\"optimizing model Not enough samples in buffer : \",buffer.__len__())\n",
    "        return\n",
    "    \n",
    "    transitions = buffer.sample(BATCH_SIZE) # batch_sized list of transitions\n",
    "    \n",
    "    state_batch = transitions[buffer.header[0]].values\n",
    "    state_batch = torch.from_numpy(np.stack( state_batch, axis=0 )).to(device)\n",
    "    #print(\"state \\n\",state_batch,\"\\n\\n\")\n",
    "\n",
    "    action_batch = torch.tensor(transitions[buffer.header[1]].values.tolist()).view(-1,1).to(device)\n",
    "    #print(\"action\\n\",action_batch,\"\\n\\n\")\n",
    "\n",
    "    next_state_batch = transitions[buffer.header[0]].values\n",
    "    next_state_batch = torch.from_numpy(np.stack( next_state_batch, axis=0 )).to(device)\n",
    "    #print(\"next_state_batch \\n\",next_state_batch,\"\\n\\n\")\n",
    "\n",
    "    reward_batch = torch.tensor(transitions[buffer.header[3]].values.tolist()).view(-1,1).to(device)\n",
    "    #print(\"\\n\\nreward \\n\",reward_batch,\"\\n\\n\")\n",
    "\n",
    "    done_batch = torch.tensor(transitions[buffer.header[4]].values.tolist()).view(-1,1).to(device)\n",
    "    #print(\"\\n\\nDone\\n\",done_batch,\"\\n\\n\")\n",
    "\n",
    "    #print(\"ravel \",state_batch.ravel())\n",
    "    qsav = qvfa(state_batch)\n",
    "    #print(\"\\n\\nstate value fn \\n\",qsav)\n",
    "\n",
    "\n",
    "\n",
    "    qsa = qvfa(state_batch).gather(1, action_batch)\n",
    "    #print(\"\\n\\nqsa \\n\",qsa)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        qvfa.eval()\n",
    "        next_state_action_values = qvfa(next_state_batch)\n",
    "        #print(\"\\n\\nnext_state_action_values\\n\", next_state_action_values)\n",
    "        max_next_state_values, _indices = next_state_action_values.max(dim=1)\n",
    "        max_next_state_values = max_next_state_values.view(-1,1)\n",
    "        #print(\"\\n\\nmax_next_state_values\\n\", max_next_state_values)\n",
    "        #print(\"\\n\\ntorch.tensor(GAMMA)\\n\", torch.tensor(GAMMA))\n",
    "        next_state_values = ((max_next_state_values*GAMMA).float()+reward_batch).float()*(1-done_batch).float()\n",
    "        #print(\"\\n\\nnext_state_values\\n\", next_state_values)\n",
    "        target = next_state_values.double()\n",
    "        qvfa.train()\n",
    "\n",
    "\n",
    "    # implement this ð›¿=ð‘„(ð‘ ,ð‘Ž)âˆ’(ð‘Ÿ+ð›¾maxð‘Žð‘„(ð‘ â€²,ð‘Ž))\n",
    "    # print(\"\\n\\ntarget\\n\",target)\n",
    "    # print(\"\\n\\nqsa\\n\",qsa)\n",
    "\n",
    "\n",
    "    loss = F.smooth_l1_loss(qsa, target)\n",
    "    # log this loss    \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in qvfa.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    ## clear_output(wait=True)\n",
    "    #print(\"optimizing model loss = \", loss.item())\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward =  12.32\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-1c68872f0da3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;31m# Optmiize after every episode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0moptimize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi_episode\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mCHECK_EVERY\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-6129a68a149e>\u001b[0m in \u001b[0;36moptimize_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m#print(\"\\n\\nstate action value \\n\",state_action_values)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mstate_action_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqvfa\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\WindowsSoftware\\Anaconda\\envs\\acme\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-323971cfaf91>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\WindowsSoftware\\Anaconda\\envs\\acme\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\WindowsSoftware\\Anaconda\\envs\\acme\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\WindowsSoftware\\Anaconda\\envs\\acme\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1402\u001b[0m         \u001b[1;33m-\u001b[0m \u001b[0mOutput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0m_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1403\u001b[0m     \"\"\"\n\u001b[1;32m-> 1404\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1405\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "done = False\n",
    "## plt.figure()\n",
    "## img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "env.render(mode='rgb_array')\n",
    "for i_episode in range(NUM_EPISODES):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    #print(\"Explore buffer size = \",buffer.__len__())\n",
    "    while not done:\n",
    "        #env.render()\n",
    "        ## img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "        env.render(mode='rgb_array')\n",
    "        ## display.display(plt.gcf())\n",
    "        ## display.clear_output(wait=True)\n",
    "        \n",
    "        action = select_action(state, ep = EPSILON)\n",
    "        #print(action,\"\\t\",action.data)\n",
    "        next_state, reward, done, info = env.step(action)        \n",
    "        #memory.push(state, action, next_state, reward, done)\n",
    "        buffer.insert(state, action, next_state, reward, done)\n",
    "        \n",
    "        ## cnt+=1\n",
    "        ## print(\"cnt\\t\",cnt)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            # this is for the terminal state\n",
    "            # no next iteration for terminal state\n",
    "            next_state = [0, 0, 0, 0]\n",
    "            buffer.insert(state, action, next_state, reward, done)\n",
    "            ## cnt+=1\n",
    "            ## print(\"cnt\\t\",cnt)\n",
    "\n",
    "    \n",
    "    # Optmiize after every episode\n",
    "    optimize_model()\n",
    "    \n",
    "    if i_episode%CHECK_EVERY == 0:\n",
    "        qvfa.eval()\n",
    "        total_reward = 0\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        for _ in range(100):\n",
    "            done = False            \n",
    "            state = env.reset()\n",
    "            while not done : \n",
    "                env.render(mode='rgb_array')\n",
    "                ## img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "                ## display.display(plt.gcf())\n",
    "                ## display.clear_output(wait=True)\n",
    "                \n",
    "                action = select_action(state)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                buffer.insert(state, action, next_state, reward, done)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    next_state = [0, 0, 0, 0]\n",
    "                    buffer.insert(state, action, next_state, reward, done)\n",
    "        total_reward /= 100\n",
    "        ## writer.add_scalar(tag_reward, total_reward, eval_count)\n",
    "        eval_count += 1\n",
    "        # Log this\n",
    "        clear_output(wait=True)\n",
    "        print(\"total reward = \", total_reward)\n",
    "        qvfa.train()\n",
    "        \n",
    "        \n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
