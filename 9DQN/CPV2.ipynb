{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "import torchvision.transforms as T\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "#plt.ion()\n",
    "\n",
    "tag_reward = \"reward\"\n",
    "tag_loss = \"loss\"\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(comment='__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "r_buff_header = ['state', 'action', 'next_state', 'reward', 'done']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.header = r_buff_header\n",
    "        self.buffer = pd.DataFrame(columns=self.header)        \n",
    "        self.count_i = 0\n",
    "        self.count_p = 0\n",
    "\n",
    "    def push(self, df_row):\n",
    "        self.count_p += 1\n",
    "        if self.__len__() == self.capacity:\n",
    "            # Probably exceeded capacity\n",
    "            #remove a row (probably 1st one) here \n",
    "            self.buffer = self.buffer.iloc[1:]\n",
    "        #add to dataframe here\n",
    "        self.buffer = pd.concat([self.buffer, df_row])\n",
    "        \n",
    "        \n",
    "    def insert(self, stateV, actonV, next_stateV, rewardV, doneV):\n",
    "        # Initialise data to lists. \n",
    "        self.count_i += 1\n",
    "        data = [{self.header[0]: stateV, \n",
    "                 self.header[1]: actonV, \n",
    "                 self.header[2]: next_stateV, \n",
    "                 self.header[3]: rewardV, \n",
    "                 self.header[4]: doneV}] \n",
    "  \n",
    "        # Creates DataFrame. \n",
    "        df = pd.DataFrame(data)\n",
    "        self.push(df)\n",
    "            \n",
    "            \n",
    "    def sample(self, batch_size=0):\n",
    "        if batch_size == 0:\n",
    "            return self.buffer.sample(self.__len__())\n",
    "        else:\n",
    "            return self.buffer.sample(batch_size)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.buffer.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DqnAgent(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_ip, n_op):\n",
    "        super(DqnAgent, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_ip, n_ip*2)\n",
    "        self.fc2 = nn.Linear(n_ip*2, n_ip)\n",
    "        self.fc3 = nn.Linear(n_ip, n_op)       \n",
    "        \n",
    "    def forward(self,x):        \n",
    "        x=self.fc1(x)\n",
    "        x=self.fc2(x)\n",
    "        x=self.fc3(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA      = 0.9 # discount factor\n",
    "EPSILON    = 0.9\n",
    "\n",
    "CHECK_EVERY = 100\n",
    "OPTIMIZE_EVERY = 1\n",
    "\n",
    "\n",
    "STATE_N  = 4\n",
    "ACTION_N = env.action_space.n\n",
    "\n",
    "# EPS_START  = 0.9\n",
    "# EPS_END    = 0.05\n",
    "# EPS_DECAY  = 200\n",
    "# TARGET_UPDATE = 10\n",
    "\n",
    "# Since target policy is absolutely greedy version of Explore policy.\n",
    "# No need of using 2 vfa nets. use 1 network.\n",
    "\n",
    "## NUM_EPISODES = 3000\n",
    "NUM_EPISODES = 3000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "## plt.figure()\n",
    "\n",
    "eval_count = 0\n",
    "train_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qvfa = DqnAgent(STATE_N, ACTION_N).double().to(device)\n",
    "optimizer = optim.Adam(qvfa.parameters())\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "## memory = ReplayMemory(10000)\n",
    "buffer = ReplayBuffer(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, ep = 0):    \n",
    "    sample = random.random()\n",
    "    state = torch.from_numpy(state).to(device)\n",
    "    if sample > ep:\n",
    "        with torch.no_grad():            \n",
    "            op = qvfa(state)\n",
    "            values, indices = op.max(0)\n",
    "            return indices.item()\n",
    "    else:\n",
    "        return env.action_space.sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    global train_count\n",
    "    \n",
    "    if buffer.__len__() < 1:\n",
    "        print(\"optimizing model Not enough samples in buffer : \",buffer.__len__())\n",
    "        return\n",
    "    \n",
    "   \n",
    "    transitions = buffer.sample(min(BATCH_SIZE, buffer.__len__()))\n",
    "        \n",
    "    \n",
    "    state_batch = transitions[buffer.header[0]].values\n",
    "    state_batch = torch.from_numpy(np.stack( state_batch, axis=0 )).to(device)\n",
    "    \n",
    "\n",
    "    action_batch = torch.tensor(transitions[buffer.header[1]].values.tolist()).view(-1,1).to(device)\n",
    "    \n",
    "\n",
    "    next_state_batch = transitions[buffer.header[0]].values\n",
    "    next_state_batch = torch.from_numpy(np.stack( next_state_batch, axis=0 )).to(device)\n",
    "    \n",
    "\n",
    "    reward_batch = torch.tensor(transitions[buffer.header[3]].values.tolist()).view(-1,1).to(device)\n",
    "    \n",
    "    done_batch = torch.tensor(transitions[buffer.header[4]].values.tolist()).view(-1,1).to(device)\n",
    "    \n",
    "    qsa = qvfa(state_batch).gather(1, action_batch)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        qvfa.eval()\n",
    "        next_state_action_values = qvfa(next_state_batch)\n",
    "        max_next_state_values, _indices = next_state_action_values.max(dim=1)\n",
    "        max_next_state_values = max_next_state_values.view(-1,1)\n",
    "        next_state_values = ((max_next_state_values*GAMMA).float()+reward_batch).float()*(1-done_batch).float()\n",
    "        target = next_state_values.double()\n",
    "        qvfa.train()\n",
    "\n",
    "\n",
    "    # 𝛿=𝑄(𝑠,𝑎)−(𝑟+𝛾max𝑎𝑄(𝑠′,𝑎))\n",
    "    \n",
    "\n",
    "    loss = criterion(qsa, target)\n",
    "    # log this loss    \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #for param in qvfa.parameters():param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    ## clear_output(wait=True)\n",
    "    # print(\"optimizing model loss = \", loss.item())\n",
    "    writer.add_scalar(tag_loss, loss.item(), train_count)\n",
    "    train_count +=1\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "done = False\n",
    "\n",
    "env.render(mode='rgb_array')\n",
    "for i_episode in range(NUM_EPISODES):\n",
    "    state = env.reset()\n",
    "    done = Falseca\n",
    "    while not done:\n",
    "        \n",
    "        env.render(mode='rgb_array')\n",
    "        action = select_action(state, ep = (EPSILON))\n",
    "        next_state, reward, done, info = env.step(action)        \n",
    "        buffer.insert(state, action, next_state, reward, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            # this is for the terminal state\n",
    "            # no next iteration for terminal state\n",
    "            next_state = [0, 0, 0, 0]\n",
    "            buffer.insert(state, action, next_state, reward, done)\n",
    "        \n",
    "    # Optmiize after every episode\n",
    "    optimize_model()\n",
    "    \n",
    "    if i_episode%CHECK_EVERY == 0:\n",
    "        if EPSILON > 0.1:\n",
    "            EPSILON -= 0.1\n",
    "        qvfa.eval()\n",
    "        total_reward = 0\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        for _ in range(100):\n",
    "            done = False            \n",
    "            state = env.reset()\n",
    "            while not done : \n",
    "                env.render(mode='rgb_array')\n",
    "                \n",
    "                action = select_action(state)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                buffer.insert(state, action, next_state, reward, done)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    next_state = [0, 0, 0, 0]\n",
    "                    buffer.insert(state, action, next_state, reward, done)\n",
    "        total_reward /= 100\n",
    "        writer.add_scalar(tag_reward, total_reward, eval_count)\n",
    "        eval_count += 1\n",
    "        qvfa.train()\n",
    "        \n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
