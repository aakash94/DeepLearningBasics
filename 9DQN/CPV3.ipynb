{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "import torchvision.transforms as T\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "tag_reward = \"reward\"\n",
    "tag_loss = \"loss\"\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(comment='__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_buff_header = ['state', 'action', 'next_state', 'reward', 'done']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.header = r_buff_header\n",
    "        self.buffer = pd.DataFrame(columns=self.header)        \n",
    "        self.count_i = 0\n",
    "        self.count_p = 0\n",
    "\n",
    "    def push(self, df_row):\n",
    "        self.count_p += 1\n",
    "        if self.__len__() == self.capacity:\n",
    "            # Probably exceeded capacity\n",
    "            #remove a row (probably 1st one) here \n",
    "            self.buffer = self.buffer.iloc[1:]\n",
    "        #add to dataframe here\n",
    "        self.buffer = pd.concat([self.buffer, df_row])\n",
    "        \n",
    "        \n",
    "    def insert(self, stateV, actonV, next_stateV, rewardV, doneV):\n",
    "        # Initialise data to lists. \n",
    "        self.count_i += 1\n",
    "        data = [{self.header[0]: stateV, \n",
    "                 self.header[1]: actonV, \n",
    "                 self.header[2]: next_stateV, \n",
    "                 self.header[3]: rewardV, \n",
    "                 self.header[4]: doneV}] \n",
    "  \n",
    "        # Creates DataFrame. \n",
    "        df = pd.DataFrame(data)\n",
    "        self.push(df)\n",
    "            \n",
    "            \n",
    "    def sample(self, batch_size=0):\n",
    "        if batch_size == 0:\n",
    "            return self.buffer.sample(self.__len__())\n",
    "        else:\n",
    "            return self.buffer.sample(batch_size)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.buffer.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DqnAgent(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_ip, n_op):\n",
    "        super(DqnAgent, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_ip, n_ip*2)\n",
    "        self.fc2 = nn.Linear(n_ip*2, n_ip*2)\n",
    "        self.fc3 = nn.Linear(n_ip*2, n_op)       \n",
    "        \n",
    "    def forward(self,x):        \n",
    "        x=self.fc1(x)\n",
    "        x=self.fc2(x)\n",
    "        x=self.fc3(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA      = 0.9 # discount factor\n",
    "EPSILON    = 1\n",
    "EPSILON_DECAY    = 0.99\n",
    "\n",
    "CHECK_EVERY = 100\n",
    "OPTIMIZE_EVERY = 1\n",
    "\n",
    "STATE_N  = 4\n",
    "ACTION_N = env.action_space.n\n",
    "\n",
    "OPTIMIZE_COUNT = 1\n",
    "\n",
    "NUM_EPISODES = 10000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "qvfa = DqnAgent(STATE_N, ACTION_N).double().to(device)\n",
    "optimizer = optim.Adam(qvfa.parameters())\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "buffer = ReplayBuffer(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, ep = 0):    \n",
    "    sample = random.random()\n",
    "    state = torch.from_numpy(state).to(device)\n",
    "    if sample > ep:\n",
    "        with torch.no_grad():            \n",
    "            op = qvfa(state)\n",
    "            values, indices = op.max(0)\n",
    "            return indices.item()\n",
    "    else:\n",
    "        return env.action_space.sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(i_episode = 0):\n",
    "\n",
    "    if buffer.__len__() < 1:\n",
    "        print(\"optimizing model Not enough samples in buffer : \",buffer.__len__())\n",
    "        return\n",
    "    \n",
    "    transitions = buffer.sample(min(BATCH_SIZE, buffer.__len__()))\n",
    "        \n",
    "    state_batch = transitions[buffer.header[0]].values\n",
    "    state_batch = torch.from_numpy(np.stack( state_batch, axis=0 )).to(device)\n",
    "    \n",
    "    action_batch = torch.tensor(transitions[buffer.header[1]].values.tolist()).view(-1,1).to(device)\n",
    "    \n",
    "    next_state_batch = transitions[buffer.header[0]].values\n",
    "    next_state_batch = torch.from_numpy(np.stack( next_state_batch, axis=0 )).to(device)\n",
    "    \n",
    "    reward_batch = torch.tensor(transitions[buffer.header[3]].values.tolist()).view(-1,1).to(device)\n",
    "    \n",
    "    done_batch = torch.tensor(transitions[buffer.header[4]].values.tolist()).view(-1,1).to(device)\n",
    "    \n",
    "    qsa = qvfa(state_batch).gather(1, action_batch)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        qvfa.eval()\n",
    "        next_state_action_values = qvfa(next_state_batch)\n",
    "        max_next_state_values, _indices = next_state_action_values.max(dim=1)\n",
    "        max_next_state_values = max_next_state_values.view(-1,1)\n",
    "        next_state_values = ((max_next_state_values*GAMMA).float()+reward_batch).float()*(1-done_batch).float()\n",
    "        target = next_state_values.double()\n",
    "        qvfa.train()\n",
    "\n",
    "\n",
    "    # ð›¿=ð‘„(ð‘ ,ð‘Ž)âˆ’(ð‘Ÿ+ð›¾maxð‘Žð‘„(ð‘ â€²,ð‘Ž))\n",
    "    loss = criterion(qsa, target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #for param in qvfa.parameters():param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    writer.add_scalar(tag_loss, loss.item(), i_episode)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\WindowsSoftware\\Anaconda\\envs\\acme\\lib\\site-packages\\ipykernel_launcher.py:17: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(NUM_EPISODES):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        env.render(mode='rgb_array')\n",
    "        action = select_action(state, ep = EPSILON)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        buffer.insert(state, action, next_state, reward, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            next_state = [0, 0, 0, 0]\n",
    "            buffer.insert(state, action, next_state, reward, done)\n",
    "        \n",
    "    writer.add_scalar(tag_reward, total_reward, i_episode)\n",
    "    for _ in range (OPTIMIZE_COUNT):\n",
    "        optimize_model(i_episode)\n",
    "    \n",
    "    if EPSILON > 0.01 :\n",
    "        EPSILON *= EPSILON_DECAY\n",
    "        \n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
